{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5610fdee-db34-4e7d-a0b1-fc8bdc62d5ed",
   "metadata": {},
   "source": [
    "## Import ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "8dbcd366-4448-4e42-bf48-7e274be2273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338ebba-f760-4cb0-b51f-e8de93ff1c79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Comment:</b> This is first version of random feature generator via function programming.\n",
    "\n",
    "After some bugfixes it will be refactored and incapsulate via class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981921e-2616-43e4-9de0-56a32349f3db",
   "metadata": {},
   "source": [
    "## Test data - Titanic from Kaggle ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b955e-7721-4486-8e36-2857ab81646d",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/titanic/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd825330-c03d-40b0-acb9-38de2fc810e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b></b> We will use only numeric columns only for testing.\n",
    "\n",
    "TO DO - cat features.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "a3bbba4f-0ee7-448d-b188-d68151d99435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Titanic/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "06b048eb-5502-4353-8e7a-ef3d226ea79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Survived': 'y'}, inplace=True) # label = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "6b792e79-d9d1-4e17-abed-3a8cb7a476f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Age','Fare','PassengerId','Pclass','y' ]] # numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "8294b43c-d4ab-4588-b023-5989f2862946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True) # leave filling na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "68a79135-b914-4a4b-9f50-81e95629e679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>39.0</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>886</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>32.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age     Fare  PassengerId  Pclass  y\n",
       "0    22.0   7.2500            1       3  0\n",
       "1    38.0  71.2833            2       1  1\n",
       "2    26.0   7.9250            3       3  1\n",
       "3    35.0  53.1000            4       1  1\n",
       "4    35.0   8.0500            5       3  0\n",
       "..    ...      ...          ...     ... ..\n",
       "885  39.0  29.1250          886       3  0\n",
       "886  27.0  13.0000          887       2  0\n",
       "887  19.0  30.0000          888       1  1\n",
       "889  26.0  30.0000          890       1  1\n",
       "890  32.0   7.7500          891       3  0\n",
       "\n",
       "[714 rows x 5 columns]"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd73616-44eb-4432-8870-0928503053bd",
   "metadata": {},
   "source": [
    "Let's set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "dac6dc70-3e70-4890-be49-ab7e028e4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 1):\n",
    "    np.random.seed(seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd75f5-0754-4f02-8b7e-dbd95e031881",
   "metadata": {},
   "source": [
    "## Fuctions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "08b382ba-ed9c-46e9-88c5-f567f621feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ray.remote\n",
    "def fit(df: pd.DataFrame, algorithm, metric, metric_highter_better: bool = True, \n",
    "        use_subsample: int = 2, feature_combiner: bool = False, EPOCHS: int = 1, \n",
    "        iterations: int = 10, replace_cols: bool = True) -> tuple:\n",
    "    \n",
    "    \"\"\" \n",
    "    Briefly, function generalize calculations from feature_creator and choose best transformations.\n",
    "    Has same parameters with feature_creator function.\n",
    "    \n",
    "    Params:\n",
    "    df: pd.DataFrame - initial data frame with feature columns and label columns with \\'y\\' name, \n",
    "    algorithm - any object, which has _.fit() and _.predict() methods, which take X, y,  \n",
    "    metric - any object (or operator), which spanned (y_pred, y_true) form to float number  $L: (y,y) -> R$, \n",
    "    metric_highter_better: bool = True - flag, which show increasing or decresing character of metric \n",
    "    (for example - mse - highter -> worse, accuracy - highter -> better), \n",
    "    use_subsample: int = 2 - num of features in one subsample for creating feature - use N cols for metric calculation, \n",
    "    feature_combiner: bool = False - combine feature in one subsample (in use_subsample) TO DO, \n",
    "    EPOCHS: int = 1 - amount of epochs, \n",
    "    iterations: int = 10 - amount of iteration for feature generation for one subsample (in connection \n",
    "    with replace_cols = False - define amount of new features (if it will be better via metric) ), \n",
    "    replace_cols: bool = True - replace initial feature or create new and concatenate ones via pd.concat(axis = 1)\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    tuple:\n",
    "        best_res_df - data frame with best features, \n",
    "        all_best_score - best score after all epochs, \n",
    "        all_best_tranformes - best tranformes after all epochs, \n",
    "        scores - list of scores\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    y = df['y']\n",
    "    df = df.drop('y', axis = 1)\n",
    "    df_shape = df.shape[1]\n",
    "    \n",
    "    if (metric_highter_better):\n",
    "        all_best_score = 0\n",
    "    else:\n",
    "        all_best_score = 1e10\n",
    "        \n",
    "    for epoch in range(0, EPOCHS):\n",
    "        res_df = pd.DataFrame([])\n",
    "        set_cols = set(range(0, df_shape))\n",
    "\n",
    "        while (len(set_cols) != 0):\n",
    "            #print(set_cols)\n",
    "            num_cols = np.random.choice(a = list(set_cols), replace = False, size = use_subsample)\n",
    "            #print(num_cols)\n",
    "\n",
    "            current_subsample, best_score, best_tranformes, scores = feature_creator(subsample = df[df.columns[num_cols]].copy(), \n",
    "                                                      y_col = y, \n",
    "                                                      algorithm_object = algorithm, \n",
    "                                                      metric = metric, \n",
    "                                                      metric_highter_better = True, \n",
    "                                                      iterations = iterations, \n",
    "                                                      replace_cols = replace_cols)\n",
    "\n",
    "            res_df = pd.concat([res_df, current_subsample], axis = 1)\n",
    "\n",
    "            for set_elem in num_cols:\n",
    "                set_cols.remove(set_elem)\n",
    "        \n",
    "        if (metric_highter_better):\n",
    "            if (best_score > all_best_score):\n",
    "                # leave changes and continue\n",
    "                all_best_score = best_score\n",
    "                #best_tranformes.append((choice_transform, subsample.columns[choice_column], best_score))\n",
    "                best_res_df = res_df.copy()\n",
    "                all_best_tranformes = best_tranformes\n",
    "        else:\n",
    "            if (best_score < all_best_score):\n",
    "                # leave changes and continue\n",
    "                all_best_score = best_score\n",
    "                #best_tranformes.append((choice_transform, subsample.columns[choice_column], best_score))\n",
    "                best_res_df = res_df.copy()\n",
    "                all_best_tranformes = best_tranformes\n",
    "       \n",
    "        \n",
    "        \n",
    "    return best_res_df, all_best_score, all_best_tranformes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "821dbb2a-5f25-49df-b7c3-e4e8e01c661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creator(subsample: pd.DataFrame, y_col: np.array, \n",
    "                    algorithm_object: object, metric: object, metric_highter_better: bool = True, \n",
    "                    iterations: int = 10, seed: int = 1, replace_cols: bool = True) -> tuple:\n",
    "    \n",
    "    \"\"\"\n",
    "    Single feature creator, which conduct iteration of sampling new transformations.\n",
    "    \n",
    "    Params:\n",
    "    subsample: pd.DataFrame - some cols from general dataset, which borrowed from fit-fuction and has shape = use_subsample, \n",
    "    y_col: np.array or pd.Series - label column, \n",
    "    algorithm_object - any object, which has _.fit() and _.predict() methods, which take X, y,  \n",
    "    metric - any object (or operator), which spanned (y_pred, y_true) form to float number  $L: (y,y) -> R$, \n",
    "    metric_highter_better: bool = True - flag, which show increasing or decresing character of metric \n",
    "    (for example - mse - highter -> worse, accuracy - highter -> better), \n",
    "    use_subsample: int = 2 - num of features in one subsample for creating feature - use N cols for metric calculation, \n",
    "    iterations: int = 10 - amount of iteration for feature generation for one subsample (in connection \n",
    "    with replace_cols = False - define amount of new features (if it will be better via metric) ), \n",
    "    replace_cols: bool = True - replace initial feature or create new and concatenate ones via pd.concat(axis = 1).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # some data transformers\n",
    "    def deg2(x):\n",
    "        return x**2\n",
    "    def deg3(x):\n",
    "        return x**3\n",
    "    def ln(x):\n",
    "        if (x == 0):\n",
    "            x = x + 1e-5\n",
    "        return np.log(x)\n",
    "    def log2(x):\n",
    "        if (x == 0):\n",
    "            x = x + 1e-5\n",
    "        return np.log2(x)\n",
    "    def log10(x):\n",
    "        if (x == 0):\n",
    "            x = x + 1e-5\n",
    "        return np.log10(x)\n",
    "    def filtered_exp(x):\n",
    "        if np.isinf(np.exp(x)):\n",
    "            # sys.float_info.max doesn't work and we don't need this\n",
    "            return 10000000\n",
    "        else:\n",
    "            return x\n",
    "    def add_dg2(x):\n",
    "        return x + x**2\n",
    "    def add_dg3(x):\n",
    "        return x + x**3\n",
    "    def add_dg2_dg3(x):\n",
    "        return x + x**2 + x**3\n",
    "    \n",
    "    type_of_transform = {'exp': filtered_exp, \n",
    "                         'ln': ln, 'log2': log2, 'log10': log10, \n",
    "                         'deg2':deg2, 'deg3':deg3, 'add_dg2': add_dg2,\n",
    "                         'add_dg3': add_dg3,'add_dg2_dg3':add_dg2_dg3,\n",
    "                         'sin': np.sin, 'cos': np.cos, }\n",
    "    \n",
    "    if (metric_highter_better):\n",
    "        best_score = 0\n",
    "    else:\n",
    "        best_score = 1e10\n",
    "    \n",
    "    init_subsample = subsample.copy()\n",
    "    best_subsample = pd.DataFrame([])\n",
    "    scores = dict()\n",
    "    best_tranformes = []#dict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for current_iter in range(0, iterations):\n",
    "        choice_transform = np.random.choice(a = list(type_of_transform.keys()))\n",
    "        choice_column = np.random.choice(a = list(range(0, subsample.shape[1])) )\n",
    "        \n",
    "        if (replace_cols):\n",
    "            subsample[subsample.columns[choice_column]] = subsample[subsample.columns[choice_column]].apply(type_of_transform[choice_transform])\n",
    "        else:\n",
    "            subsample[subsample.columns[choice_column]+'_{}'.format(current_iter)] = subsample[subsample.columns[choice_column]].apply(type_of_transform[choice_transform])\n",
    "        \n",
    "        #print(choice_transform)\n",
    "        subsample = subsample.replace([np.inf, -np.inf], np.nan)\n",
    "        # log can create \n",
    "        #print(subsample.isna().any().any())\n",
    "        if subsample.isna().any().any():\n",
    "            subsample = subsample.fillna(value = 0)\n",
    "        #    subsample = pd.concat([subsample, y_col], axis = 1)\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(subsample, \n",
    "                                                            y_col, \n",
    "                                                            test_size=0.33, \n",
    "                                                            shuffle=False, \n",
    "                                                            random_state=seed)\n",
    "        alg_instance = algorithm_object()\n",
    "        alg_instance.fit(X = X_train, y = y_train)\n",
    "        \n",
    "        y_pred = alg_instance.predict(X_test)\n",
    "        score = metric(y_test, y_pred)\n",
    "        #print(score)\n",
    "        if (metric_highter_better):\n",
    "            if (score > best_score):\n",
    "                # leave changes and continue\n",
    "                best_score = score\n",
    "                best_tranformes.append((choice_transform, subsample.columns[choice_column], best_score))\n",
    "            else:\n",
    "                # return transform\n",
    "                if (replace_cols):\n",
    "                    subsample[subsample.columns[choice_column]] = init_subsample[subsample.columns[choice_column]]\n",
    "                else:\n",
    "                    subsample = subsample.drop(subsample.columns[choice_column]+'_{}'.format(current_iter), axis = 1)\n",
    "\n",
    "                \n",
    "        else:\n",
    "            if (score < best_score):\n",
    "                # leave changes and continue\n",
    "                best_score = score\n",
    "                best_tranformes.append((choice_transform, subsample.columns[choice_column], best_score))\n",
    "            else:\n",
    "                # return transform\n",
    "                if (replace_cols):\n",
    "                    subsample[subsample.columns[choice_column]] = init_subsample[subsample.columns[choice_column]]\n",
    "                else:\n",
    "                    subsample = subsample.drop(subsample.columns[choice_column]+'_{}'.format(current_iter), axis = 1)\n",
    "\n",
    "                \n",
    "                \n",
    "        scores[choice_transform] = score\n",
    "    \n",
    "    return subsample, best_score, best_tranformes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "05293e7c-a7be-4ac0-add0-30ec431de334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7330508474576272,\n",
       " [('deg2', 'Pclass', 0.7330508474576272)],\n",
       " {'add_dg2': 0.635593220338983, 'sin': 0.690677966101695})"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df, bs, bt, sc = fit(df = df, algorithm = LogisticRegression, metric = accuracy_score, metric_highter_better = True, \n",
    "        use_subsample = 2, replace_cols = False, iterations = 3, EPOCHS = 3)\n",
    "#print(res_df.shape[1])\n",
    "#print(bs)\n",
    "X_train, X_test, y_train, y_test = train_test_split(res_df, df['y'], test_size=0.33, random_state=1)\n",
    "algorithm = LogisticRegression()\n",
    "algorithm.fit(X = X_train, y = y_train)\n",
    "y_pred = algorithm.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "bs, bt, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ded238-ce0c-4aa2-938d-484aafc71154",
   "metadata": {},
   "source": [
    "Исходный алгоритм на исходных данных и новый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "cda8a03d-bc1b-4d8b-901b-af49d719d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(df, alg, metric):\n",
    "    \"\"\"\n",
    "    Let's compare initial dataset and new transformed dataset using same methods from scikit-learn.\n",
    "    Return tuple:\n",
    "    (score_1 - initial metric value, score_2 - new metric value)\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis = 1), df['y'], test_size=0.33, random_state=1)\n",
    "    alg_instance = alg()\n",
    "    alg_instance.fit(X = X_train, y = y_train)\n",
    "    y_pred = alg_instance.predict(X_test)\n",
    "    score_1 = metric(y_test, y_pred)\n",
    "    \n",
    "    res_df, bs, bt, sc = fit(df = df, algorithm = alg, metric = metric, metric_highter_better = True, \n",
    "        use_subsample = 2, replace_cols = False, iterations = 3, EPOCHS = 15)\n",
    "    #print(res_df.shape[1])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(res_df, df['y'], test_size=0.33, random_state=1)\n",
    "    alg_instance = alg()\n",
    "    alg_instance.fit(X = X_train, y = y_train)\n",
    "    y_pred = alg_instance.predict(X_test)\n",
    "    score_2 = metric(y_test, y_pred)\n",
    "    \n",
    "    print(score_1, score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "e716f72f-9b57-4c4d-88d0-d2f435083b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7288135593220338 0.7033898305084746\n"
     ]
    }
   ],
   "source": [
    "compare(df, LogisticRegression, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "8d86d768-5b7d-45dc-8996-896fbe2b02f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6059322033898306 0.6949152542372882\n"
     ]
    }
   ],
   "source": [
    "compare(df, KNeighborsClassifier, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "bc9dd1c1-1df8-41d0-8d00-4a8a6f25d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7161016949152542 0.7372881355932204\n"
     ]
    }
   ],
   "source": [
    "compare(df, RandomForestClassifier, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c81db-af7a-4ff2-b2d1-fc40455879c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8b227-2db8-4b4d-94d2-fbdc7eb30335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15f23e-d1d0-400b-b740-7b9bc2de485a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae99b1b-c048-49be-be2a-754b54ae0ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
